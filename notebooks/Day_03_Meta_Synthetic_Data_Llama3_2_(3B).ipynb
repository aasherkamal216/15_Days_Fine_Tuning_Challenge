{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UvDvj4MpxyDr"
   },
   "source": [
    "## 🚀 Day 3/15 — Fine-Tuning with Unsloth AI\n",
    "\n",
    "## Synthetic Data Generation using Llama 3.2 3B model\n",
    "\n",
    "> This notebook outlines the process of generating synthetic data using the LLama 3.2-3B model and fine-tuning another model on that data.\n",
    "---\n",
    "\n",
    "### 👋🏻 About Me\n",
    "\n",
    "Hi, I'm **Aasher Kamal** — a Generative & Agentic AI developer passionate about building intelligent systems with LLMs.\n",
    "\n",
    "I have started a **15-day challenge** to master fine-tuning using the open-source **Unsloth AI** framework. This journey will cover everything from LoRA and QLoRA to reinforcement learning, vision, and TTS fine-tuning — all hands-on, all open-source.\n",
    "\n",
    "I'll be documenting my learnings, experiments, and challenges daily.\n",
    "\n",
    "---\n",
    "\n",
    "### 🌐 Connect with Me\n",
    "\n",
    "- [LinkedIn](https://www.linkedin.com/in/aasher-kamal/)\n",
    "- [GitHub](https://github.com/aasherkamal216)\n",
    "- [X (Twitter)](https://x.com/Aasher_Kamal)\n",
    "- [Facebook](https://www.facebook.com/aasher.kamal)\n",
    "- [Website](https://aasherkamal.framer.website/)\n",
    "\n",
    "Let’s build and learn together! 💡\n",
    "\n",
    "---\n",
    "\n",
    "### Acknowledgements\n",
    "\n",
    "This notebook is adapted from Unsloth's official [GitHub repository](https://github.com/unslothai/notebooks).  \n",
    "I've made minor modifications to the original version to better understand and document the workflow.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imwJhjjYPSr0"
   },
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth vllm==0.8.5.post1\n",
    "    !pip install synthetic-data-kit==0.0.3\n",
    "else:\n",
    "    !pip install --no-deps unsloth vllm==0.8.5.post1\n",
    "    !pip install synthetic-data-kit==0.0.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Colab Extra Install { display-mode: \"form\" }\n",
    "%%capture\n",
    "import os\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth vllm\n",
    "else:\n",
    "    !pip install --no-deps unsloth vllm==0.8.5.post1\n",
    "    # Skip restarting message in Colab\n",
    "    import sys, re, requests; modules = list(sys.modules.keys())\n",
    "    for x in modules: sys.modules.pop(x) if \"PIL\" in x or \"google\" in x else None\n",
    "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
    "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
    "\n",
    "    # vLLM requirements - vLLM breaks Colab due to reinstalling numpy\n",
    "    f = requests.get(\"https://raw.githubusercontent.com/vllm-project/vllm/refs/heads/main/requirements/common.txt\").content\n",
    "    with open(\"vllm_requirements.txt\", \"wb\") as file:\n",
    "        file.write(re.sub(rb\"(transformers|numpy|xformers)[^\\n]{1,}\\n\", b\"\", f))\n",
    "    !pip install -r vllm_requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j9Epwvh47VBt"
   },
   "source": [
    "### Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 07-30 09:24:39 [importing.py:53] Triton module has been replaced with a placeholder.\n",
      "INFO 07-30 09:24:39 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bab474078ba24505b7851df0cc5e85e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/890 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e98099c892eb40b68a553eea44672a4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "364f5a9efd784b4ba8510c31f52acc5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cad733bb8f33498baef59645f20fbd25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c31196859ce3409da0cfdb41f307423a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Patching vLLM v1 graph capture\n",
      "Unsloth: Patching vLLM v0 graph capture\n",
      "Unsloth: Using dtype = torch.bfloat16 for vLLM.\n",
      "Unsloth: vLLM loading unsloth/Llama-3.2-3B-Instruct with actual GPU utilization = 59.5%\n",
      "Unsloth: Your GPU has CUDA compute capability 8.9 with VRAM = 22.16 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 1024. Num Sequences = 192.\n",
      "Unsloth: vLLM's KV Cache can use up to 7.2 GB. Also swap space = 4 GB.\n",
      "vLLM STDOUT: INFO 07-30 09:25:02 [__init__.py:239] Automatically detected platform cuda.\n",
      "vLLM STDOUT: INFO 07-30 09:25:08 [api_server.py:1043] vLLM API server version 0.8.5.post1\n",
      "vLLM STDOUT: INFO 07-30 09:25:08 [api_server.py:1044] args: Namespace(subparser='serve', model_tag='unsloth/Llama-3.2-3B-Instruct', config='', host=None, port=8000, uvicorn_log_level='info', disable_uvicorn_access_log=False, allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, enable_ssl_refresh=False, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='unsloth/Llama-3.2-3B-Instruct', task='auto', tokenizer=None, hf_config_path=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, load_format='auto', download_dir=None, model_loader_extra_config={}, use_tqdm_on_load=True, config_format=<ConfigFormat.AUTO: 'auto'>, dtype='bfloat16', max_model_len=1024, guided_decoding_backend='auto', reasoning_parser=None, logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, data_parallel_size=1, enable_expert_parallel=False, max_parallel_loading_workers=None, ray_workers_use_nsight=False, disable_custom_all_reduce=False, block_size=None, gpu_memory_utilization=0.5949896167850041, swap_space=4.0, kv_cache_dtype='auto', num_gpu_blocks_override=None, enable_prefix_caching=True, prefix_caching_hash_algo='builtin', cpu_offload_gb=0, calculate_kv_scales=False, disable_sliding_window=False, use_v2_block_manager=True, seed=0, max_logprobs=0, disable_log_stats=True, quantization=None, rope_scaling=None, rope_theta=None, hf_token=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config={}, limit_mm_per_prompt={}, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=None, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=None, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', speculative_config=None, ignore_patterns=[], served_model_name=None, qlora_adapter_name_or_path=None, show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, max_num_batched_tokens=1024, max_num_seqs=192, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, num_lookahead_slots=0, scheduler_delay_factor=0.0, preemption_mode=None, num_scheduler_steps=1, multi_step_stream_outputs=True, scheduling_policy='fcfs', enable_chunked_prefill=None, disable_chunked_mm_input=False, scheduler_cls='vllm.core.scheduler.Scheduler', override_neuron_config=None, override_pooler_config=None, compilation_config={\"level\":3,\"splitting_ops\":[]}, kv_transfer_config=None, worker_cls='auto', worker_extension_cls='', generation_config='auto', override_generation_config=None, enable_sleep_mode=False, additional_config=None, enable_reasoning=False, disable_cascade_attn=False, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, enable_server_load_tracking=False, dispatch_function=<function ServeSubcommand.cmd at 0x78a260689620>)\n",
      "vLLM STDOUT: INFO 07-30 09:25:22 [config.py:717] This model supports multiple tasks: {'reward', 'generate', 'classify', 'embed', 'score'}. Defaulting to 'generate'.\n",
      "vLLM STDOUT: INFO 07-30 09:25:22 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=1024.\n",
      "vLLM STDOUT: INFO 07-30 09:25:33 [__init__.py:239] Automatically detected platform cuda.\n",
      "vLLM STDOUT: INFO 07-30 09:25:36 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='unsloth/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='unsloth/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=1024, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=unsloth/Llama-3.2-3B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
      "vLLM STDOUT: WARNING 07-30 09:25:36 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7d0465818c10>\n",
      "vLLM STDOUT: INFO 07-30 09:25:37 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "vLLM STDOUT: INFO 07-30 09:25:37 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "vLLM STDOUT: WARNING 07-30 09:25:37 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "vLLM STDOUT: INFO 07-30 09:25:37 [gpu_model_runner.py:1329] Starting to load model unsloth/Llama-3.2-3B-Instruct...\n",
      "vLLM STDOUT: INFO 07-30 09:25:38 [weight_utils.py:265] Using model weights format ['*.safetensors']\n",
      "vLLM STDOUT: INFO 07-30 09:25:59 [weight_utils.py:281] Time spent downloading weights for unsloth/Llama-3.2-3B-Instruct: 21.679091 seconds\n",
      "vLLM STDOUT: INFO 07-30 09:26:01 [loader.py:458] Loading weights took 1.86 seconds\n",
      "vLLM STDOUT: INFO 07-30 09:26:02 [gpu_model_runner.py:1347] Model loading took 6.0160 GiB and 24.684146 seconds\n",
      "vLLM STDOUT: INFO 07-30 09:26:12 [backends.py:420] Using cache directory: /root/.cache/vllm/torch_compile_cache/2171342691/rank_0_0 for vLLM's torch.compile\n",
      "vLLM STDOUT: INFO 07-30 09:26:12 [backends.py:430] Dynamo bytecode transform time: 10.19 s\n",
      "vLLM STDOUT: INFO 07-30 09:26:16 [backends.py:136] Cache the graph of shape None for later use\n",
      "vLLM STDOUT: INFO 07-30 09:26:47 [backends.py:148] Compiling a graph for general shape takes 34.36 s\n",
      "vLLM STDOUT: INFO 07-30 09:26:56 [monitor.py:33] torch.compile takes 44.54 s in total\n",
      "vLLM STDOUT: INFO 07-30 09:26:57 [kv_cache_utils.py:634] GPU KV cache size: 54,624 tokens\n",
      "vLLM STDOUT: INFO 07-30 09:26:57 [kv_cache_utils.py:637] Maximum concurrency for 1,024 tokens per request: 53.34x\n",
      "vLLM STDOUT: INFO 07-30 09:27:33 [gpu_model_runner.py:1686] Graph capturing finished in 35 secs, took 0.44 GiB\n",
      "vLLM STDOUT: INFO 07-30 09:27:33 [core.py:159] init engine (profile, create kv cache, warmup model) took 91.10 seconds\n",
      "vLLM STDOUT: INFO 07-30 09:27:33 [core_client.py:439] Core engine process 0 ready.\n",
      "vLLM STDOUT: WARNING 07-30 09:27:33 [config.py:1239] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.\n",
      "vLLM STDOUT: INFO 07-30 09:27:33 [serving_chat.py:118] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}\n",
      "vLLM STDOUT: INFO 07-30 09:27:33 [serving_completion.py:61] Using default completion sampling params from model: {'temperature': 0.6, 'top_p': 0.9}\n",
      "vLLM STDOUT: INFO 07-30 09:27:33 [api_server.py:1090] Starting vLLM API server on http://0.0.0.0:8000\n",
      "\n",
      "--- vLLM Server Ready (Detected: 'Starting vLLM API server on') ---\n"
     ]
    }
   ],
   "source": [
    "from unsloth.dataprep import SyntheticDataKit\n",
    "\n",
    "generator = SyntheticDataKit.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-3B-Instruct\",\n",
    "    max_seq_length = 1024,\n",
    "    gpu_memory_utilization=0.6  # using 60% of the total GPU memory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ef_MnK575tr2"
   },
   "source": [
    "## Generate QA Pairs + Auto clean data\n",
    "We now use synthetic data kit for question answer pair generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.prepare_qa_generation(\n",
    "    output_folder = \"data\", # Output location of synthetic data\n",
    "    temperature = 0.9, # Higher temp makes more diverse datases\n",
    "    top_p = 0.95,\n",
    "    overlap = 64, # Overlap portion during chunking\n",
    "    max_generation_tokens = 256,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yV7DyufR51IN"
   },
   "source": [
    "Check if it succeeded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l\u001b[32m VLLM server is running at \u001b[0m\u001b[4;94mhttp://localhost:8000/v1\u001b[0m\n",
      "\u001b[32m⠋\u001b[0m\u001b[32m Checking VLLM server at http://localhost:8000/v1...\u001b[0m\r",
      "\u001b[2KAvailable models: \u001b[1m{\u001b[0m\u001b[32m'object'\u001b[0m: \u001b[32m'list'\u001b[0m, \u001b[32m'data'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m{\u001b[0m\u001b[32m'id'\u001b[0m: \n",
      "\u001b[32m'unsloth/Llama-3.2-3B-Instruct'\u001b[0m, \u001b[32m'object'\u001b[0m: \u001b[32m'model'\u001b[0m, \u001b[32m'created'\u001b[0m: \u001b[1;36m1753867667\u001b[0m, \n",
      "\u001b[32m'owned_by'\u001b[0m: \u001b[32m'vllm'\u001b[0m, \u001b[32m'root'\u001b[0m: \u001b[32m'unsloth/Llama-3.2-3B-Instruct'\u001b[0m, \u001b[32m'parent'\u001b[0m: \u001b[3;35mNone\u001b[0m, \n",
      "\u001b[32m'max_model_len'\u001b[0m: \u001b[1;36m1024\u001b[0m, \u001b[32m'permission'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m{\u001b[0m\u001b[32m'id'\u001b[0m: \n",
      "\u001b[32m'modelperm-32241f8566c4453c9fcfb80fc2004228'\u001b[0m, \u001b[32m'object'\u001b[0m: \u001b[32m'model_permission'\u001b[0m, \n",
      "\u001b[32m'created'\u001b[0m: \u001b[1;36m1753867667\u001b[0m, \u001b[32m'allow_create_engine'\u001b[0m: \u001b[3;91mFalse\u001b[0m, \u001b[32m'allow_sampling'\u001b[0m: \u001b[3;92mTrue\u001b[0m, \n",
      "\u001b[32m'allow_logprobs'\u001b[0m: \u001b[3;92mTrue\u001b[0m, \u001b[32m'allow_search_indices'\u001b[0m: \u001b[3;91mFalse\u001b[0m, \u001b[32m'allow_view'\u001b[0m: \u001b[3;92mTrue\u001b[0m, \n",
      "\u001b[32m'allow_fine_tuning'\u001b[0m: \u001b[3;91mFalse\u001b[0m, \u001b[32m'organization'\u001b[0m: \u001b[32m'*'\u001b[0m, \u001b[32m'group'\u001b[0m: \u001b[3;35mNone\u001b[0m, \u001b[32m'is_blocking'\u001b[0m: \n",
      "\u001b[3;91mFalse\u001b[0m\u001b[1m}\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\n",
      "\u001b[32m⠋\u001b[0m Checking VLLM server at http://localhost:8000/v1...\r",
      "\u001b[2K\u001b[32m⠋\u001b[0m Checking VLLM server at http://localhost:8000/v1...\n",
      "\u001b[?25h\r",
      "\u001b[1A\u001b[2K"
     ]
    }
   ],
   "source": [
    "!synthetic-data-kit system-check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xdl7aPFK55M1"
   },
   "source": [
    "## Document Parsing\n",
    "I have placed the document `comprehensive_guide_daca.md` manually inside `data/output/` folder. We'll use this file and covert it to Q&A pairs in order to finetune Llama 3.2!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98 ['data/output/comprehensive_guide_daca_0.md', 'data/output/comprehensive_guide_daca_1.md', 'data/output/comprehensive_guide_daca_2.md']\n"
     ]
    }
   ],
   "source": [
    "# Truncate document\n",
    "filenames = generator.chunk_data(\"data/output/comprehensive_guide_daca.md\")\n",
    "print(len(filenames), filenames[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nGdAXafV6S2M"
   },
   "source": [
    "We see around 98 chunks of data. We now call synthetic-data-kit to create some pairs of data for 10 of our chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2KProcessing 3 chunks to generate QA pairs...\n",
      "\u001b[2KBatch processing complete.\n",
      "\u001b[2KGenerated 6 QA pairs total\n",
      "\u001b[2KSaving result to data/generated/comprehensive_guide_daca_0_qa_pairs.json\n",
      "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
      "\u001b[2KSuccessfully wrote result to \n",
      "data/generated/comprehensive_guide_daca_0_qa_pairs.json\n",
      "\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from data/output/comprehensive_guide_daca_0.md...\n",
      "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/comprehensive_guide_daca_0_qa_pairs.json\u001b[0m\n",
      "\u001b[2KProcessing 3 chunks to generate QA pairs...\n",
      "\u001b[2KBatch processing complete.\n",
      "\u001b[2KGenerated 8 QA pairs total\n",
      "\u001b[2KSaving result to data/generated/comprehensive_guide_daca_1_qa_pairs.json\n",
      "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
      "\u001b[2KSuccessfully wrote result to \n",
      "data/generated/comprehensive_guide_daca_1_qa_pairs.json\n",
      "\u001b[2K\u001b[32m⠼\u001b[0m Generating qa content from data/output/comprehensive_guide_daca_1.md...\n",
      "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/comprehensive_guide_daca_1_qa_pairs.json\u001b[0m\n",
      "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
      "\u001b[2KBatch processing complete.\n",
      "\u001b[2KGenerated 6 QA pairs total\n",
      "\u001b[2KSaving result to data/generated/comprehensive_guide_daca_2_qa_pairs.json\n",
      "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
      "\u001b[2KSuccessfully wrote result to \n",
      "data/generated/comprehensive_guide_daca_2_qa_pairs.json\n",
      "\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from data/output/comprehensive_guide_daca_2.md...\n",
      "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/comprehensive_guide_daca_2_qa_pairs.json\u001b[0m\n",
      "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
      "\u001b[2KBatch processing complete.\n",
      "\u001b[2KGenerated 8 QA pairs total\n",
      "\u001b[2KSaving result to data/generated/comprehensive_guide_daca_3_qa_pairs.json\n",
      "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
      "\u001b[2KSuccessfully wrote result to \n",
      "data/generated/comprehensive_guide_daca_3_qa_pairs.json\n",
      "\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from data/output/comprehensive_guide_daca_3.md...\n",
      "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/comprehensive_guide_daca_3_qa_pairs.json\u001b[0m\n",
      "\u001b[2KProcessing 2 chunks to generate QA pairs...\n",
      "\u001b[2KBatch processing complete.\n",
      "\u001b[2KGenerated 10 QA pairs total\n",
      "\u001b[2KSaving result to data/generated/comprehensive_guide_daca_4_qa_pairs.json\n",
      "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
      "\u001b[2KSuccessfully wrote result to \n",
      "data/generated/comprehensive_guide_daca_4_qa_pairs.json\n",
      "\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from data/output/comprehensive_guide_daca_4.md...\n",
      "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/comprehensive_guide_daca_4_qa_pairs.json\u001b[0m\n",
      "\u001b[2KProcessing 4 chunks to generate QA pairs...\n",
      "\u001b[2KBatch processing complete.\n",
      "\u001b[2KGenerated 0 QA pairs total\n",
      "\u001b[2KSaving result to data/generated/comprehensive_guide_daca_5_qa_pairs.json\n",
      "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
      "\u001b[2KSuccessfully wrote result to \n",
      "data/generated/comprehensive_guide_daca_5_qa_pairs.json\n",
      "\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from data/output/comprehensive_guide_daca_5.md...\n",
      "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/comprehensive_guide_daca_5_qa_pairs.json\u001b[0m\n",
      "\u001b[31mL Error: VLLM server not available at \u001b[0m\u001b[4;94mhttp://localhost:8000/v1\u001b[0m\n",
      "\u001b[33mPlease start the VLLM server with:\u001b[0m\n",
      "\u001b[1;34mvllm serve unsloth/Llama-\u001b[0m\u001b[1;36m3.2\u001b[0m\u001b[1;34m-3B-Instruct\u001b[0m\n",
      "\u001b[31mL Error: VLLM server not available at \u001b[0m\u001b[4;94mhttp://localhost:8000/v1\u001b[0m\n",
      "\u001b[33mPlease start the VLLM server with:\u001b[0m\n",
      "\u001b[1;34mvllm serve unsloth/Llama-\u001b[0m\u001b[1;36m3.2\u001b[0m\u001b[1;34m-3B-Instruct\u001b[0m\n",
      "\u001b[31mL Error: VLLM server not available at \u001b[0m\u001b[4;94mhttp://localhost:8000/v1\u001b[0m\n",
      "\u001b[33mPlease start the VLLM server with:\u001b[0m\n",
      "\u001b[1;34mvllm serve unsloth/Llama-\u001b[0m\u001b[1;36m3.2\u001b[0m\u001b[1;34m-3B-Instruct\u001b[0m\n",
      "\u001b[31mL Error: VLLM server not available at \u001b[0m\u001b[4;94mhttp://localhost:8000/v1\u001b[0m\n",
      "\u001b[33mPlease start the VLLM server with:\u001b[0m\n",
      "\u001b[1;34mvllm serve unsloth/Llama-\u001b[0m\u001b[1;36m3.2\u001b[0m\u001b[1;34m-3B-Instruct\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "for filename in filenames[:10]:\n",
    "    !synthetic-data-kit \\\n",
    "        -c synthetic_data_kit_config.yaml \\\n",
    "        create {filename} \\\n",
    "        --num-pairs 10 \\\n",
    "        --type \"qa\"\n",
    "    time.sleep(2) # Sleep some time to leave some room for processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xqYpdXa7znZ6"
   },
   "source": [
    "**Note:**  We are currently encountering a VLLM server error. Attempts to resolve the issue by upgrading the GPU and system memory have not been successful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AScJ5-vAOjYj"
   },
   "source": [
    "We now convert the generated datasets into QA formats so we can load it for finetuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l\u001b[32m⠋\u001b[0m Converting data/generated/comprehensive_guide_daca_0_qa_pairs.json to ft \n",
      "format with json storage...\n",
      "\u001b[?25h\r",
      "\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[32m Converted to ft format and saved to \u001b[0m\n",
      "\u001b[1;32mdata/final/comprehensive_guide_daca_0_qa_pairs_ft.json\u001b[0m\n",
      "\u001b[?25l\u001b[32m⠋\u001b[0m Converting data/generated/comprehensive_guide_daca_1_qa_pairs.json to ft \n",
      "format with json storage...\n",
      "\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[32m Converted to ft format and saved to \u001b[0m\n",
      "\u001b[1;32mdata/final/comprehensive_guide_daca_1_qa_pairs_ft.json\u001b[0m\n",
      "\u001b[?25l\u001b[32m⠋\u001b[0m Converting data/generated/comprehensive_guide_daca_2_qa_pairs.json to ft \n",
      "format with json storage...\n",
      "\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[32m Converted to ft format and saved to \u001b[0m\n",
      "\u001b[1;32mdata/final/comprehensive_guide_daca_2_qa_pairs_ft.json\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "qa_pairs_filenames = [\n",
    "    f\"data/generated/comprehensive_guide_daca_{i}_qa_pairs.json\"\n",
    "    for i in range(len(filenames[:3]))\n",
    "]\n",
    "for filename in qa_pairs_filenames:\n",
    "    !synthetic-data-kit \\\n",
    "        -c synthetic_data_kit_config.yaml \\\n",
    "        save-as {filename} -f ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6dVK-qza7rPB"
   },
   "source": [
    "Let's load up the data and see what the synthetic data looks like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "final_filenames = [\n",
    "    f\"data/final/comprehensive_guide_daca_{i}_qa_pairs_ft.json\"\n",
    "    for i in range(len(filenames[:3]))\n",
    "]\n",
    "conversations = pd.concat([\n",
    "    pd.read_json(name) for name in final_filenames\n",
    "]).reset_index(drop = True)\n",
    "\n",
    "dataset = Dataset.from_pandas(conversations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'content': 'You are a helpful assistant.', 'role': 'system'},\n",
       "  {'content': 'What is the potential benefit of leveraging free-tier cloud services in planetary-scale production?',\n",
       "   'role': 'user'},\n",
       "  {'content': 'cost optimization', 'role': 'assistant'}]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'content': 'You are a helpful assistant.', 'role': 'system'},\n",
       "  {'content': 'What are the key features of DACA framework for agentic AI applications?',\n",
       "   'role': 'user'},\n",
       "  {'content': 'a robust, flexible, and cost-effective framework',\n",
       "   'role': 'assistant'}]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aO9qePmP7yaY"
   },
   "source": [
    "Finally free vLLM process to save memory and to allow for finetuning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to terminate the VLLM server gracefully...\n",
      "Server did not terminate gracefully after 10 seconds. Forcing kill...\n",
      "Server killed forcefully.\n"
     ]
    }
   ],
   "source": [
    "generator.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EQo2PR7oqDQE"
   },
   "source": [
    "### Fine-tuning Synthetic Dataset with Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.7.11: Fast Llama patching. Transformers: 4.54.0. vLLM: 0.8.5.post1.\n",
      "   \\\\   /|    NVIDIA L4. Num GPUs = 1. Max memory: 22.161 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d9897f75a7f464fa3532b1baf1cb08e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.35G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6b99a8ea05241b3a6031c5740087972",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/234 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fd1f9d6691249ab921fb1cd6b2a012a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f36582d0acdc4c998213cdfc9fe24420",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92745bb9fd5944e69fe47ab75713fa71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb48a492862243e5ad92bc25a23d42ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-3B-Instruct\",\n",
    "    max_seq_length = 1024,\n",
    "    load_in_4bit = True,  # 4 bit quantization to reduce memory\n",
    "    full_finetuning = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXd9bTZd1aaL"
   },
   "source": [
    "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.7.11 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vITh0KVJ10qX"
   },
   "source": [
    "<a name=\"Data\"></a>\n",
    "### Data Prep\n",
    "We now use the `Llama-3.2` format for conversation style finetunes. The chat template renders conversations like below: (Cutting Knowledge Date is by default there!)\n",
    "\n",
    "```\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today Date: 01 May 2025\n",
    "\n",
    "You are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "What is 1+1?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "2<|eot_id|>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c961d22992a8407b8adabff12152b6e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"messages\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "    return {\"text\" : texts}\n",
    "\n",
    "# Get our previous dataset and format it:\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'content': 'You are a helpful assistant.', 'role': 'system'},\n",
       "  {'content': 'What are the foundational tenets of DACA?', 'role': 'user'},\n",
       "  {'content': 'The foundational tenets of DACA are AI-first and cloud-first.',\n",
       "   'role': 'assistant'}],\n",
       " 'text': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 30 Jul 2025\\n\\nYou are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhat are the foundational tenets of DACA?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nThe foundational tenets of DACA are AI-first and cloud-first.<|eot_id|>'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[13]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yBdfVXJU1V45"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a9450e5e4464166a9131ce57ec8ea9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    eval_dataset = None,\n",
    "    args = SFTConfig(\n",
    "        dataset_text_field = \"text\",\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 60,\n",
    "        learning_rate = 2e-4,\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        report_to = \"none\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA L4. Max memory = 22.161 GB.\n",
      "3.07 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 20 | Num Epochs = 20 | Total steps = 60\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 24,313,856 of 3,237,063,680 (0.75% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 01:13, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.783000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.736800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.902100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.534000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4.125900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.652500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.293500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.537000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.290100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.090400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.858000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.688200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.821300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.416200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.463500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.676800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.168900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.461400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.215500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.266600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.158700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.104800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.237300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.004300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.027500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.852600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.811500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.842400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.884600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.687700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.725900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.745600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.629800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.599600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.594600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.571800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.480700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.489800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.445300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.442000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.442400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.375800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.416800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.346100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.355200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.351100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.318500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.316600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.322700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.308800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.311000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.282000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.288000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.285700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.267100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.272000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.248300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.275100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.272800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79.8495 seconds used for training.\n",
      "1.33 minutes used for training.\n",
      "Peak reserved memory = 3.07 GB.\n",
      "Peak reserved memory for training = 0.0 GB.\n",
      "Peak reserved memory % of max memory = 13.853 %.\n",
      "Peak reserved memory for training % of max memory = 0.0 %.\n"
     ]
    }
   ],
   "source": [
    "# @title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(\n",
    "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
    ")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekOmTR1hSNcr"
   },
   "source": [
    "<a name=\"Inference\"></a>\n",
    "### Inference\n",
    "Let's run the model! Use `apply_chat_template` with `add_generation_prompt` set to `True` for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The purpose of DACA is to implement Agentia World.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is the purpose of DACA?\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer,\n",
    "                   max_new_tokens = 256, temperature = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The foundations of DACA are AI-first and cloud-first.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What are the foundations of DACA?\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer,\n",
    "                   max_new_tokens = 256, temperature = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cT3ns9S61x75"
   },
   "source": [
    "## Observations\n",
    "\n",
    "The synthetic data generated by LLama 3.2 is of poor quality. This appears to be due to the low values set for `max_seq_length` and `max_generation_tokens`, which were intentionally limited to conserve GPU memory. Attempts to use higher values led to rapid memory exhaustion.\n",
    "\n",
    "Additionally, we encountered VLLM server issues during QA pairs generation, specifically when generating multiple QA pairs. The VLLM server tends to shut down unexpectedly after some time.\n",
    "\n",
    "As a result, the QA pairs produced by LLama 3.2 have contributed to poor fine-tuning outcomes."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
