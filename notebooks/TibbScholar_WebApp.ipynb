{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1433e7cf6e48449bb8fd7735f72858f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"Aasher/TibbScholar\", device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "What are the risks in dental implant surgery?\n",
      "\n",
      "Answer:\n",
      "The risks in dental implant surgery include infection, which can occur after surgery due to the creation of a new surface and wound drainage through the skin. Additionally, the placement of screws with a pointed head and short handle creates a risk of bone fracturing, which can result in a fractured screw. To reduce the risks, the placement of implants can be performed in an upright and parallel manner, using a round head screw with a handle or a tapered screw with a handle that is straightened for insertion.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Question:\n",
    "What are the risks in dental implant surgery?\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "response = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=1)\n",
    "print(response[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found GPU! Using device: cuda:0\n",
      "Loading TibbScholar model... This may take a moment.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb8f753ffc784a8cba9bc09e6e25390a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ TibbScholar model loaded successfully!\n",
      "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
      "* Running on public URL: https://d822c82b79404138ac.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://d822c82b79404138ac.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# --- 1. Load the Model and Pipeline (Done once on startup) ---\n",
    "\n",
    "# Check if a GPU is available and set the device accordingly.\n",
    "# This makes the app work on both GPU and CPU systems.\n",
    "try:\n",
    "    device = 0 if torch.cuda.is_available() else -1\n",
    "    print(f\"‚úÖ Found GPU! Using device: cuda:0\")\n",
    "except Exception:\n",
    "    device = -1\n",
    "    print(f\"‚ö†Ô∏è No GPU found. Using device: CPU\")\n",
    "\n",
    "# Load your fine-tuned model from the Hugging Face Hub.\n",
    "# This pipeline object will be reused for every prediction.\n",
    "print(\"Loading TibbScholar model... This may take a moment.\")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"Aasher/TibbScholar\",\n",
    "    device=device,\n",
    "    torch_dtype=torch.bfloat16 # Use bfloat16 for better performance on modern GPUs\n",
    ")\n",
    "print(\"‚úÖ TibbScholar model loaded successfully!\")\n",
    "\n",
    "\n",
    "# --- 2. Define the Prediction Function ---\n",
    "\n",
    "def get_tibb_response(user_question):\n",
    "    \"\"\"\n",
    "    This function takes a user's question, formats it into the prompt,\n",
    "    gets a prediction from the model, and cleans up the output.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = f\"\"\"Question:\n",
    "{user_question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "    # Generate the response using the pipeline\n",
    "    response = pipe(\n",
    "        prompt,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        temperature=0.5,\n",
    "        top_p=0.95\n",
    "    )\n",
    "\n",
    "    # The output includes the original prompt, so we need to extract only the generated part.\n",
    "    full_text = response[0][\"generated_text\"]\n",
    "\n",
    "    # Split the text at \"Answer:\" and take the second part.\n",
    "    answer_part = full_text.split(\"Answer:\")[1].strip()\n",
    "\n",
    "    return answer_part\n",
    "\n",
    "\n",
    "# --- 3. Create and Launch the Gradio Interface ---\n",
    "\n",
    "with gr.Blocks(theme=gr.themes.Soft(), title=\"TibbScholar\") as app:\n",
    "    # Add a title and description in Markdown\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        # ü©∫ TibbScholar: Your Medical AI Assistant\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    # Input component\n",
    "    question_input = gr.Textbox(\n",
    "        label=\"Your Medical Question\",\n",
    "        placeholder=\"e.g., What is the difference between Type 1 and Type 2 diabetes?\"\n",
    "    )\n",
    "\n",
    "    # Output component\n",
    "    answer_output = gr.Textbox(\n",
    "        label=\"TibbScholar's Answer\",\n",
    "        lines=10,\n",
    "        interactive=False # User cannot edit the output\n",
    "    )\n",
    "\n",
    "    # Button to submit the question\n",
    "    submit_button = gr.Button(\"Get Answer\", variant=\"primary\")\n",
    "\n",
    "    # Define some example questions to make it easy for users to try\n",
    "    gr.Examples(\n",
    "        [\n",
    "            \"What risk factors are associated with abdominal aortic aneurysm rupture?\",\n",
    "            \"Is tumor budding a factor in gastric cancer?\",\n",
    "            \"What is the difference between Type 1 and Type 2 diabetes?\",\n",
    "            \"What are the early symptoms of Parkinson's disease?\",\n",
    "        ],\n",
    "        inputs=question_input,\n",
    "        outputs=answer_output,\n",
    "        fn=get_tibb_response,\n",
    "    )\n",
    "\n",
    "    # Connect the button click event to the prediction function\n",
    "    submit_button.click(\n",
    "        fn=get_tibb_response,\n",
    "        inputs=question_input,\n",
    "        outputs=answer_output\n",
    "    )\n",
    "\n",
    "# Launch the app! If you are running this in Google Colab,\n",
    "# set share=True to get a public link.\n",
    "app.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
