{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SAsh1UVgR5Xc"
   },
   "source": [
    "## ðŸš€ Day 5/15 â€” Fine-Tuning with Unsloth AI\n",
    "# TibbScholar: A Medical AI Assistant\n",
    "\n",
    "### Project Overview\n",
    "\n",
    "This notebook documents the process of creating **TibbScholar**, a specialized Large Language Model (LLM) designed to function as a knowledgeable and reliable medical assistant.\n",
    "\n",
    "By fine-tuning a powerful base model on a vast corpus of medical question-answer pairs, we aim to build an AI that can provide accurate, context-aware information for medical students, researchers, and professionals. The project leverages the Unsloth library for highly efficient, memory-optimized training on a single GPU.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oyhXNqBNxZvh"
   },
   "source": [
    "### ðŸ› ï¸ Technology Stack & Key Components\n",
    "\n",
    "*   **Base Model:** Meta's Llama 3.2 (3B) - `unsloth/Llama-3.2-3B-unsloth-bnb-4bit`. Chosen for its strong instruction-following capabilities and robust performance in the 3B parameter class.\n",
    "*   **Fine-tuning Framework:** Unsloth AI, for 2x faster training and up to 70% less memory usage.\n",
    "*   **Technique:** QLoRA (4-bit Quantized Low-Rank Adaptation) to efficiently fine-tune the model with limited VRAM.\n",
    "*   **Dataset:** MIRIAD (`miriad/miriad-4.4M`), a comprehensive collection of medical QA pairs, ideal for instilling deep domain knowledge.\n",
    "*   **Platform:** Google Colab Pro with an NVIDIA A100 GPU.\n",
    "---\n",
    "\n",
    "### ðŸ“‹ Dataset\n",
    "\n",
    "To teach **TibbScholar** to act as a reliable assistant, we need to structure our training data in a clear and consistent format.\n",
    "\n",
    "#### Dataset\n",
    "We are using the `miriad/miriad-4.4M` dataset, which contains about millions of medical question-answer pairs. For this training run, we will use a randomly selected subset of **100,000** examples to balance training time and performance. The data is streamed and shuffled to ensure the model sees a diverse and representative sample.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAUN6FgMgDRx"
   },
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
    "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
    "    !pip install --no-deps unsloth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pKOpKR9QgDR0"
   },
   "source": [
    "### Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.7.11: Fast Llama patching. Transformers: 4.54.1.\n",
      "   \\\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.557 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a46bee7eb1674683ac31a269246b9691",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.35G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff2a2f09a5ab47449806f7b03e8b461e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/230 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0c7181fef464ec1a53dd82d1a1ede1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f118777e3f0c47ffabfbf993c0fe0371",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/459 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdc1fdaf19c54cbea7fd3b45c37bffc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 2048\n",
    "dtype = None\n",
    "load_in_4bit =  True\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-3B-unsloth-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXd9bTZd1aaL"
   },
   "source": [
    "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.7.11 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 32,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3407,\n",
    "    loftq_config = None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"miriad/miriad-4.4M\", split = \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0788e578f2c14562b48080fe7516733a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset prepared and formatted.\n",
      "First example:\n",
      "<|begin_of_text|>Question:\n",
      "What are the symptoms and clinical manifestations of oculomotor dysfunction in patients with motor neuron disease?\n",
      "\n",
      "\n",
      "Answer:\n",
      "Oculomotor dysfunction in patients with motor neuron disease can manifest as difficulty in voluntary opening or closing of the eyelids. Patients may experience a conscious and sustained effort to close their eyes, and may develop a habit of gently closing the eyelids with their fingers. However, their eyelids remain closed normally during sleep. Other symptoms of motor neuron disease, such as slurred speech, difficulty in swallowing, muscle atrophy, weakness, and fasciculation in the upper limbs and shoulders, may also be present. Cognitive dysfunction, including attention and recall impairment, may occur. Neurological examination may reveal both upper and lower motor neuron signs with bulbar involvement. Reflex blinking to various stimuli is typically normal, but voluntary initiation of eyelid closure is impaired.<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "# 1. Define the prompt format\n",
    "alpaca_prompt = \"\"\"Question:\n",
    "{}\n",
    "\n",
    "Answer:\n",
    "{}\"\"\"\n",
    "\n",
    "# 2. Create the formatting function\n",
    "def formatting_prompts_func(examples):\n",
    "    questions = examples[\"question\"]\n",
    "    answers = examples[\"answer\"]\n",
    "    texts = []\n",
    "    for question, answer in zip(questions, answers):\n",
    "        text = alpaca_prompt.format(question, answer)\n",
    "        texts.append(tokenizer.bos_token + text + tokenizer.eos_token)\n",
    "    return { \"text\" : texts }\n",
    "\n",
    "dataset = dataset.shuffle(seed=40).select(range(100_000))\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "print(\"Dataset prepared and formatted.\")\n",
    "print(f\"First example:\\n{dataset[0]['text']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Question:\n",
      "What are the risk factors for nerve damage during dental implant surgery?\n",
      "\n",
      "\n",
      "Answer:\n",
      "The main risk factors for nerve damage during dental implant surgery include errors in evaluation and planning, errors in injection of local anesthetic, bone preparation mistakes, and placement errors of the implant. Other factors that may increase the risk include inadequate maintenance of oral hygiene, plaque accumulation, and misdirection of supra structure. These factors can contribute to implant failure and increase the likelihood of nerve damage.<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "import random; print(dataset[random.randint(1, 100000)]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idAEIeSQ3xdS"
   },
   "source": [
    "<a name=\"Train\"></a>\n",
    "### Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    packing = False,\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 500,\n",
    "        max_steps = 10000,\n",
    "        learning_rate = 2e-4,\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"cosine\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train(resume_from_checkpoint = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(\n",
    "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
    ")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekOmTR1hSNcr"
   },
   "source": [
    "<a name=\"Inference\"></a>\n",
    "### Inference\n",
    "Let's run the model! You can change the instruction and input - leave the output blank!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "How does chronic exercise training impact in the elderly??\n",
      "\n",
      "Answer:\n",
      "Chronic exercise training has been shown to improve functional status and muscle mass in the elderly. It also increases muscle fiber cross-sectional area and has a positive effect on the rate of decline in muscle strength.\n"
     ]
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "prompt_template = \"\"\"Question:\n",
    "{}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "question = \"How does chronic exercise training impact in the elderly??\"\n",
    "\n",
    "# Format the prompt\n",
    "prompt = prompt_template.format(question)\n",
    "\n",
    "inputs = tokenizer(\n",
    "    [prompt],\n",
    "    return_tensors = \"pt\"\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens = 512,\n",
    "    use_cache = True,\n",
    "    do_sample = True,\n",
    "    temperature = 0.7,\n",
    "    top_p = 0.9\n",
    "    )\n",
    "\n",
    "response_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "\n",
    "# Print the result\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Question:\n",
      "What is oculomotor dysfunction? What are its symptoms?\n",
      "\n",
      "Answer:\n",
      "Oculomotor dysfunction is a condition characterized by impaired movement of the eye. It can result from various causes, including injuries, diseases, or neurological disorders. Symptoms of oculomotor dysfunction may include impaired eye movement, double vision, inability to focus on objects, and impaired pupil response to light. In the case of oculomotor palsy, the symptoms may be more severe and can lead to difficulty in performing daily activities.<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "inputs = tokenizer(\n",
    "    [prompt_template.format(\"What is oculomotor dysfunction? What are its symptoms?\")],\n",
    "    return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Question:\n",
      "What are the early and late-stage changes observed in hip osteoarthritis?\n",
      "\n",
      "Answer:\n",
      "Early-stage changes in hip osteoarthritis include increased cartilage thickness, increased cartilage volume, and increased cartilage density. Late-stage changes include reduced cartilage thickness and volume, and reduced cartilage density.<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "    [prompt_template.format(\"What are the early and late-stage changes observed in hip osteoarthritis?\")],\n",
    "    return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wYHTxaQ4_OTN"
   },
   "source": [
    "### Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "hf_token = userdata.get('HF_TOKEN')\n",
    "\n",
    "# Merge to 16bit\n",
    "model.push_to_hub_merged(\"Aasher/TibbScholar\", tokenizer, save_method = \"merged_16bit\", token = hf_token, maximum_memory_usage=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FZSVk0cC-dQ2"
   },
   "source": [
    "### Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    \"Aasher/TibbScholar\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Question:\n",
      "What are the risks in dental implant surgery?\n",
      "\n",
      "Answer:\n",
      "The risks in dental implant surgery can include complications such as infection, failure to heal, or complications related to the implant placement. These risks should be clearly discussed with the patient and documented in the patient's records.<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "prompt = \"\"\"Question:\n",
    "What are the risks in dental implant surgery?\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "inputs = tokenizer(\n",
    "    [prompt],\n",
    "    return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, temperature=1, max_new_tokens = 256)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
