{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 Day 8/15 — Fine-Tuning with Unsloth AI\n",
    "\n",
    "## Project: Fine-Tuning Gemma 3n (2B) for Image-to-Prompt Generation\n",
    "\n",
    "This notebook demonstrates how to fine-tune a powerful multimodal model, Google's Gemma 3n, to perform a specific and creative task: generating descriptive text-to-image prompts from an input image.\n",
    "\n",
    "### 🎯 Project Goal\n",
    "\n",
    "The primary objective is to create an \"AI art deconstruction\" tool. Given an image, the fine-tuned model should analyze its visual content, style, and composition, then generate a high-quality text prompt that could be used by a text-to-image model (like Dall-E-3 or Midjourney) to create a similar image.\n",
    "\n",
    "### 💾 The Dataset\n",
    "\n",
    "We are using a subset of the **`jackyhate/text-to-image-2M`** dataset available on Hugging Face.\n",
    "\n",
    "*   **Content:** This dataset contains 2 million pairs of images and the high-quality text prompts used to generate them. This is the perfect data source for our task, as it provides direct examples of the image-to-prompt relationship we want the model to learn.\n",
    "*   **Format:** The data is provided in the `webdataset` format (`.tar` files), which is optimized for large-scale streaming.\n",
    "*   **Preprocessing:** For this demonstration, we perform the following steps:\n",
    "    1.  Stream the dataset directly from Hugging Face.\n",
    "    2.  Take a small subset of **2,000 examples** to ensure the process is fast and manageable within the Colab environment.\n",
    "    3.  Transform each sample into a conversational format suitable for training.\n",
    "\n",
    "You can find the full dataset here: [https://huggingface.co/datasets/jackyhate/text-to-image-2M](https://huggingface.co/datasets/jackyhate/text-to-image-2M)\n",
    "\n",
    "---\n",
    "### 👋🏻 About Me\n",
    "\n",
    "Hi, I'm **Aasher Kamal** — a Generative & Agentic AI developer passionate about building intelligent systems with LLMs.\n",
    "\n",
    "I have started a **15-day challenge** to master fine-tuning using the open-source **Unsloth AI** framework. This journey will cover everything from LoRA and QLoRA to reinforcement learning, vision, and TTS fine-tuning — all hands-on, all open-source.\n",
    "\n",
    "I'll be documenting my learnings, experiments, and challenges daily.\n",
    "\n",
    "---\n",
    "\n",
    "### 🌐 Connect with Me\n",
    "\n",
    "- [LinkedIn](https://www.linkedin.com/in/aasher-kamal/)\n",
    "- [GitHub](https://github.com/aasherkamal216)\n",
    "- [X (Twitter)](https://x.com/Aasher_Kamal)\n",
    "- [Facebook](https://www.facebook.com/aasher.kamal)\n",
    "- [Website](https://aasherkamal.framer.website/)\n",
    "\n",
    "Let’s build and learn together! 💡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
    "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
    "    !pip install --no-deps unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastVisionModel\n",
    "import torch\n",
    "\n",
    "model, processor = FastVisionModel.from_pretrained(\n",
    "    \"unsloth/gemma-3n-E2B-it-unsloth-bnb-4bit\",\n",
    "    load_in_4bit = True, \n",
    "    max_seq_length = 2048,\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastVisionModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers     = True,\n",
    "    finetune_language_layers   = True,\n",
    "    finetune_attention_modules = True,\n",
    "    finetune_mlp_modules       = True,\n",
    "\n",
    "    r = 32,\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    random_state = 42,\n",
    "    use_rslora = False,\n",
    "    loftq_config = None,\n",
    "    target_modules = \"all-linear\",\n",
    "    modules_to_save=[\n",
    "        \"lm_head\",\n",
    "        \"embed_tokens\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "# 1. Load the streaming dataset\n",
    "base_url = \"https://huggingface.co/datasets/jackyhate/text-to-image-2M/resolve/main/data_512_2M/data_{i:06d}.tar\"\n",
    "num_shards = 46\n",
    "urls = [base_url.format(i=i) for i in range(num_shards)]\n",
    "full_dataset_stream = load_dataset(\"webdataset\", data_files={\"train\": urls}, split=\"train\", streaming=True)\n",
    "\n",
    "# 2. Take a subset and load it into a Python list in memory.\n",
    "subset_list = list(full_dataset_stream.take(2000))\n",
    "\n",
    "# 3. Define your formatting function\n",
    "instruction = \"Generate a detailed, descriptive prompt for this image, suitable for a text-to-image model.\"\n",
    "\n",
    "def convert_to_conversation(sample):\n",
    "    image = sample[\"jpg\"]\n",
    "    prompt_text = sample[\"json\"][\"prompt\"]\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": instruction},\n",
    "                {\"type\": \"image\", \"image\": image}\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": prompt_text}\n",
    "            ]\n",
    "        },\n",
    "    ]\n",
    "    # Return the dictionary in the format the trainer expects\n",
    "    return {\"messages\": messages}\n",
    "\n",
    "# 4. Use a list comprehension to create the final processed list\n",
    "dataset = [convert_to_conversation(sample) for sample in subset_list]\n",
    "\n",
    "# Let's verify that we have a live PIL Image object, NOT bytes\n",
    "first_image_object = dataset[0]['messages'][0]['content'][1]['image']\n",
    "print(\"Type of the image object in our final list:\", type(first_image_object))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import get_chat_template\n",
    "\n",
    "processor = get_chat_template(\n",
    "    processor,\n",
    "    \"gemma-3n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.trainer import UnslothVisionDataCollator\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "FastVisionModel.for_training(model) # Enable for training!\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    processing_class=processor.tokenizer,\n",
    "    data_collator=UnslothVisionDataCollator(model, processor),\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        gradient_checkpointing = True,\n",
    "\n",
    "        # use reentrant checkpointing\n",
    "        gradient_checkpointing_kwargs = {\"use_reentrant\": False},\n",
    "        max_grad_norm = 0.3,              # max gradient norm based on QLoRA paper\n",
    "        warmup_ratio = 0.1,\n",
    "        num_train_epochs = 1,\n",
    "        learning_rate = 2e-4,\n",
    "        logging_steps = 1,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps = 100,\n",
    "        save_total_limit = 3,\n",
    "        optim = \"adamw_torch_fused\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"cosine\",\n",
    "        seed = 42,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\",             # For Weights and Biases\n",
    "\n",
    "        # You MUST put the below items for vision finetuning:\n",
    "        remove_unused_columns = False,\n",
    "        dataset_text_field = \"\",\n",
    "        dataset_kwargs = {\"skip_prepare_dataset\": True},\n",
    "        max_length = 2048,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9IBMUWgTFQeR"
   },
   "source": [
    "----\n",
    "\n",
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "FastVisionModel.for_inference(model)\n",
    "\n",
    "image_url = \"https://substackcdn.com/image/fetch/$s_!fCoi!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F81899f64-aaae-469c-9463-6f8be3f6b2ab_1920x1080.jpeg\"\n",
    "response = requests.get(image_url)\n",
    "image = Image.open(BytesIO(response.content))\n",
    "\n",
    "instruction = \"Generate a detailed, descriptive prompt for this image, suitable for a text-to-image model.\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": instruction}],\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "\n",
    "inputs = processor(\n",
    "    image,\n",
    "    input_text,\n",
    "    add_special_tokens=False,\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "text_streamer = TextStreamer(processor, skip_prompt=True)\n",
    "\n",
    "result = model.generate(\n",
    "    **inputs,\n",
    "    streamer=text_streamer,\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.8,\n",
    "    top_p=0.9,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "hf_token = userdata.get('HF_TOKEN')\n",
    "\n",
    "model.push_to_hub_merged(\"Aasher/Image2Prompt_Generator_Gemma_3n_2B\", processor, token = hf_token)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
